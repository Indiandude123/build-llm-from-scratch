{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters are :  I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "The total number of characters in this text is :  20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(\"The first 100 characters are : \", text[:99])\n",
    "\n",
    "print(\"The total number of characters in this text is : \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'World.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "samp_text = \"Hello, World. This, is a test.\"\n",
    "result = re.split('(\\s)', samp_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets say we want to also split the commas and the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'World', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'World', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split('([.,]|\\s)', samp_text)\n",
    "print(result)\n",
    "\n",
    "new_result =[]\n",
    "for item in result:\n",
    "    if(item.strip()):\n",
    "        new_result.append(item)\n",
    "print(new_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'World', '!', '', ' ', 'I', ' ', 'am', ' ', 'new', ' ', 'to', ' ', 'this', '--', '', ' ', 'do', ' ', 'you', ' ', 'know', ' ', 'how', ' ', 'to', ' ', 'code', '?', '']\n"
     ]
    }
   ],
   "source": [
    "samp_text2 = \"Hello, World! I am new to this-- do you know how to code?\"\n",
    "result = re.split('([,.?/!;:\"()\\']|--|\\s)', samp_text2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets proceed to tokenzing the verdict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split('([,.?/!;:\"()_\\']|--|\\s)', text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we will assign token IDs to these tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(set(preprocessed))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for idx, token in enumerate(tokens):\n",
    "    vocab[token] = idx\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = self.create_inverse_vocab(vocab)\n",
    "    \n",
    "    def create_inverse_vocab(self, vocab):\n",
    "        inv_dict = {}\n",
    "        for key, value in vocab.items():\n",
    "            inv_dict[value] = key\n",
    "        return inv_dict\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        preprocessed_sent = re.split('([,.?/!;:\"()_\\']|--|\\s)', sentence)\n",
    "        preprocessed_sent = [item.strip() for item in preprocessed_sent if item.strip()]\n",
    "        token_ids_sent = [self.str_to_int[token] for token in preprocessed_sent]\n",
    "        return token_ids_sent\n",
    "        \n",
    "    def decode(self, token_id_sentence):\n",
    "        sentence = \" \".join([self.int_to_str[token_id] for token_id in token_id_sentence])\n",
    "        sentence = re.sub(r'\\s+([,.?/!;:\"()_\\']|--)', r'\\1', sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[988, 5, 235, 656, 584, 500, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "sentence = \"the, brown man is good.\"\n",
    "tokenized_sent = tokenizer.encode(sentence)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the, brown man is good.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But there is one limitation, OOV words need to be handled. Also if multiple sources are used to generate the corpus, we might want to keep the end of sentence token in between each. So, we'll modify the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(set(preprocessed))\n",
    "tokens.extend(['<UNK>', '<EOS>'])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for idx, token in enumerate(tokens):\n",
    "    vocab[token] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('younger', 1127),\n",
       " ('your', 1128),\n",
       " ('yourself', 1129),\n",
       " ('<UNK>', 1130),\n",
       " ('<EOS>', 1131)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = self.create_inverse_vocab(vocab)\n",
    "    \n",
    "    def create_inverse_vocab(self, vocab):\n",
    "        inv_dict = {}\n",
    "        for key, value in vocab.items():\n",
    "            inv_dict[value] = key\n",
    "        return inv_dict\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        preprocessed_sent = re.split('([,.?/!;:\"()_\\']|--|\\s)', sentence)\n",
    "        preprocessed_sent = [item.strip() for item in preprocessed_sent if item.strip()]\n",
    "        preprocessed_sent2 = []\n",
    "        for word in preprocessed_sent:\n",
    "            if word not in self.str_to_int:\n",
    "                preprocessed_sent2.append(\"<UNK>\")\n",
    "            else:\n",
    "                preprocessed_sent2.append(word)\n",
    "        token_ids_sent = [self.str_to_int[token] for token in preprocessed_sent2]\n",
    "        return token_ids_sent\n",
    "        \n",
    "    def decode(self, token_id_sentence):\n",
    "        sentence = \" \".join([self.int_to_str[token_id] for token_id in token_id_sentence])\n",
    "        sentence = re.sub(r'\\s+([,.?/!;:\"()_\\']|--)', r'\\1', sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[988, 5, 235, 1130, 584, 500, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "sentence = \"the, brown fox is good.\"\n",
    "tokenized_sent = tokenizer.encode(sentence)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the, brown <UNK> is good.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <EOS> In the sunlight terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlight terraces of the palace\"\n",
    "\n",
    "text = \" <EOS> \".join([text1, text2])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 1130, 984, 722, 988, 1130]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = tokenizer.encode(text)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>, do you like tea? <EOS> In the <UNK> terraces of the <UNK>'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
